model:
  hubert_model_name: "Atotti/Google-USM"
  hubert_layer: 12
  adapter_hidden_dim: 1536  # Adapterの中間層の次元数

dataset:
  path_pattern:
    - /home/ayu/datasets/jvs_preprocessed/jvs-train-{000000..000025}.tar.gz
    - /home/ayu/datasets/fleurs-r_preprocessed/fleurs-r-train-{000000..000910}.tar.gz
    - /home/ayu/datasets/libritts_r_preprocessed/libritts_r-train-{000000..000504}.tar.gz
  val_path_pattern:
    - /home/ayu/datasets/jvs_preprocessed/jvs-val-{000000..000001}.tar.gz
  shuffle: 1000          # WebDataset 内部 shuffle バッファ
batch_size: 16
steps: 2000000
validation_interval: 10000
validation_batches: 16

training:
  gradient_accumulation_steps: 1
  mixed_precision: "no"
  dataloader_drop_last: true

optim:
  lr: 2.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  max_grad_norm: 1.0
  scheduler:
    name: "cosine_with_restarts"
    warmup_steps: 10000
    first_cycle_steps: 100000  # 最初のサイクルの長さ
    cycle_mult: 1.0           # サイクル長の倍率
    max_lr: 2.0e-4            # 最大学習率
    min_lr: 1.0e-6            # 最小学習率

loader:
  num_workers: 8
  pin_memory: true

save_dir: exp/usm  # チェックポイント保存先
log_interval: 1000        # iter ごとに損失表示

# Checkpoint configuration
checkpoint:
  save_interval: 10000      # 1kステップごとにチェックポイント保存
  resume_from: null        # 再開用チェックポイントパス
  keep_last_n: 500          # 最新N個のチェックポイントを保持
  save_wandb_metadata: true # wandb情報も保存

# Wandb logging configuration
wandb:
  enabled: true
  project: "miipher-2-adapter"
  entity: null             # デフォルトのwandbエンティティを使用
  name: null               # 実行名を自動生成
  tags: ["usm", "adapter", "training"]
  notes: "Parallel Adapter training for Miipher-2"
  log_model: false
