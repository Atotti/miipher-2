model:
  hubert_model_name: "utter-project/mHuBERT-147"
  hubert_layer: 6
  adapter_hidden_dim: 768

dataset:
  path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-train-{000000..000020}.tar.gz
  val_path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-val-{000000..000001}.tar.gz
  shuffle: 1000

batch_size: 2
loader:
  num_workers: 8
  pin_memory: true

adapter_ckpt: exp/adapter_layer_6_mhubert_147/checkpoint_159k.pt
pretrained_gen: exp/hifigan_pretrain_layer_6_mhubert_147/checkpoint_96k.pt

# ----- Generator アーキテクチャ -----
upsample_rates:        [8, 8, 2, 2]
upsample_kernel_sizes: [16, 16, 4, 4]
resblock_kernel_sizes: [3, 7, 11]
resblock_dilations: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]

# ----- 学習ハイパーパラメータ -----
steps: 200000
validation_interval: 1000
validation_batches: 25

optim:
  lr: 2.0e-4 # 初期LR
  betas: [0.9, 0.95]
  scheduler:
    name: "cosine"
    warmup_steps: 5000
    min_lr_ratio: 0.1 # 最低 LR = initial_lr × 0.1
  max_grad_norm: 1.0 # Gradient Clippingの上限値
  r1_lambda: 10.0      # R1 Regularizationの係数
  ema_decay: 0.999     # Generator EMAの減衰率

# 損失関数の重み
lambda_mel: 25.0
lambda_adv: 2.0
lambda_fm: 1.0 # Feature Matching Lossの重み (HiFi-GANの元実装では2倍がハードコードされているため1.0で等価)

save_dir: exp/hifigan_ft_layer_6_mhubert_147
log_interval: 100    # iter ごとにログ

# Checkpoint configuration
checkpoint:
  save_interval: 1000
  resume_from: null      # 再開用チェックポイントパス
  keep_last_n: 500       # 最新N個のチェックポイントを保持
  save_wandb_metadata: true # wandb情報も保存

# Wandb logging configuration
wandb:
  enabled: true
  project: "miipher-2-hifigan-finetune"
  entity: null           # デフォルトのwandbエンティティを使用
  name: null             # 実行名を自動生成
  tags: ["hifigan", "vocoder", "finetune"]
  notes: "HiFi-GAN fine-tuning for Miipher-2"
  log_model: true
  log_audio: true        # 音声サンプルをログ
