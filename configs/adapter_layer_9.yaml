model:
  hubert_model_name: "rinna/japanese-hubert-large"  # HuBERTベースモデル名
  hubert_layer: 9          # 使用するHuBERTの層（0-based）
  adapter_hidden_dim: 1024  # Adapterの中間層の次元数

dataset:
  path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-train-{000000..000020}.tar.gz
  val_path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-val-{000000..000001}.tar.gz
  shuffle: 1000          # WebDataset 内部 shuffle バッファ
batch_size: 8
steps: 60000
validation_interval: 1000
validation_batches: 25

training:
  gradient_accumulation_steps: 2
  mixed_precision: "no"
  dataloader_drop_last: true

optim:
  lr: 2.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  max_grad_norm: 1.0
  scheduler:
    name: "constant_with_warmup"
    warmup_steps: 100

loader:
  num_workers: 8
  pin_memory: true

save_dir: exp/adapter_layer_9
log_interval: 100        # iter ごとに損失表示

# Checkpoint configuration
checkpoint:
  save_interval: 1000      # 1kステップごとにチェックポイント保存
  resume_from: null        # 再開用チェックポイントパス
  keep_last_n: 500          # 最新N個のチェックポイントを保持
  save_wandb_metadata: true # wandb情報も保存

# Wandb logging configuration
wandb:
  enabled: true
  project: "miipher-2-adapter"
  entity: null             # デフォルトのwandbエンティティを使用
  name: null               # 実行名を自動生成
  tags: ["hubert", "adapter", "training"]
  notes: "Parallel Adapter training for Miipher-2"
  log_model: false
