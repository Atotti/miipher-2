# Model configuration
model:
  hubert_model_name: "rinna/japanese-hubert-large"  # HuBERTベースモデル名
  hubert_layer: 12          # 使用するHuBERTの層（0-based）
  adapter_hidden_dim: 1024  # Adapterの中間層の次元数

dataset:
  path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-train-{000000..000020}.tar.gz
  shuffle: 1000          # WebDataset 内部 shuffle バッファ
batch_size: 8            # GPU1枚あたり
epochs: 5

optim:
  lr: 2.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  max_grad_norm: 1.0  # 勾配クリッピングの最大ノルム

loader:
  num_workers: 8
  pin_memory: true

save_dir: exp/adapter
log_interval: 100        # iter ごとに損失表示

# Wandb logging configuration
wandb:
  enabled: true
  project: "miipher-2-adapter"
  entity: null             # デフォルトのwandbエンティティを使用
  name: null               # 実行名を自動生成
  tags: ["adapter", "training"]
  notes: "Parallel Adapter training for Miipher-2"
  log_model: false
