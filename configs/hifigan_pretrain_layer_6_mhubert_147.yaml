model:
  hubert_model_name: "utter-project/mHuBERT-147"
  hubert_layer: 6
  adapter_hidden_dim: null # 事前学習ではAdapterは使用しない

dataset:
  path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-train-{000000..000020}.tar.gz
  val_path_pattern: /home/ayu/datasets/jvs_preprocessed/jvs-val-{000000..000001}.tar.gz
  shuffle: 1000

batch_size: 2
loader:
  num_workers: 8
  pin_memory: true

# 事前学習では学習済みAdapterは不要
adapter_ckpt: null

# 公式の事前学習済みモデルをベースに学習する
pretrained_gen: assets/universal_hifigan_v1/g_02500000

# ----- Generator アーキテクチャ -----
upsample_rates:        [8, 8, 2, 2]
upsample_kernel_sizes: [16, 16, 4, 4]
resblock_kernel_sizes: [3, 7, 11]
resblock_dilations: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]

# ----- 学習ハイパーパラメータ -----
steps: 100000
validation_interval: 1000
validation_batches: 25

optim:
  lr: 2.0e-4 # 初期LR
  betas: [0.9, 0.95]
  scheduler:
    name: "cosine"
    warmup_steps: 5000
    min_lr_ratio: 0.1 # 最低 LR = initial_lr × 0.1

lambda_stft: 1.0 # 不要かも
lambda_mpd: 2.5
lambda_msd: 2.5

# チェックポイントの保存先を事前学習用に変更
save_dir: exp/hifigan_pretrain_layer_6_mhubert_147
log_interval: 100

# Checkpoint configuration
checkpoint:
  save_interval: 1000
  resume_from: null
  keep_last_n: 500
  save_wandb_metadata: true

# Wandb logging configuration
wandb:
  enabled: true
  project: "miipher-2-hifigan-pretrain"
  entity: null
  name: null
  tags: ["hifigan", "vocoder", "pretrain"]
  notes: "HiFi-GAN pre-training on clean HuBERT features"
  log_model: true
  log_audio: true
